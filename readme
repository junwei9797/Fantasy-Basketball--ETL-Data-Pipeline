End-to-End Data Pipeline (ETL) for Fantasy Basketball
Tech stack: Airflow, Python (Pandas), PostgreSQL, Docker
Data flow:
    1) Ingest nba score data via NBA API
    2) Transform using Pandas
    3) Store in PostgreSQL
    Bonus: Visualize with Tableau

Pipeline is deployed on Docker, and scheduled to run daily with Airflow
(Apache airflow will create spin up docker containers to run the job, doing so makes it easier to integrate to Kubernetes in the future)


DAG Name	                Populates	                        Schedule	                                        Dependencies
extract_teams	            team	                            On season start	                                    none
extract_players	            player,                             player_team (if separate) On season start	        extract_teams
extract_games	            game	                            Daily or hourly	                                    extract_teams
extract_stats	            fantasy_score_per_game, boxscore	Daily after games	                                extract_games, extract_players
aggregate_career	        player_career_stats	                Daily/weekly	                                    extract_stats